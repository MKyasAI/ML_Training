{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2608ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load data -------------------------------------------------\n",
    "\n",
    "# Create 10 evenly spaced points between 0 and 2Ï€\n",
    "x = np.linspace(0, 2*np.pi, 10)\n",
    "\n",
    "# Generate sinusoidal y values\n",
    "y = np.sin(x)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# features and targets\n",
    "X_all = df[\"x\"].values.reshape(-1, 1).astype(\"float32\")\n",
    "y_all = df[\"y\"].values.astype(\"float32\")\n",
    "\n",
    "# 2. Split into train / val / test ----------------------------\n",
    "# 60% train, 20% val, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.4, random_state=40 #try random_state=42 for a difficult distribution\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c508215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build a simple regression NN in Keras ---------------------\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)  # linear output for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\"  # mean squared error\n",
    ")\n",
    "# 4. Train -----------------------------------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    verbose=1  # set to 1 if you want progress output\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, frameon=False)\n",
    "plt.grid(True, linewidth=0.8, alpha=0.6)\n",
    "plt.title('Training and Validation Loss', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate --------------------------------------------------\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "y_pred_val   = model.predict(X_val).flatten()\n",
    "y_pred_test  = model.predict(X_test).flatten()\n",
    "y_pred_all   = model.predict(X_all).flatten()\n",
    "\n",
    "x_pred_lin  = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_pred_lin   = model.predict(x_pred_lin).flatten()\n",
    "\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_val   = mean_absolute_error(y_val,   y_pred_val)\n",
    "mae_test  = mean_absolute_error(y_test,  y_pred_test)\n",
    "\n",
    "print(f\"MAE train: {mae_train:.3f}\")\n",
    "print(f\"MAE val:   {mae_val:.3f}\")\n",
    "print(f\"MAE test:  {mae_test:.3f}\")\n",
    "# 6. Plot Experimental Data vs Model Data NN ------------------\n",
    "# Sort by time for a nice continuous line\n",
    "sort_idx = np.argsort(X_all.flatten())\n",
    "t_sorted = X_all.flatten()[sort_idx]\n",
    "y_sorted = y_all[sort_idx]\n",
    "y_pred_sorted = y_pred_all[sort_idx]\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(X_train.flatten(), y_train,\n",
    "         label=\"Training Data\",\n",
    "         marker='o',\n",
    "         linestyle='',\n",
    "         markersize=10,\n",
    "         color='blue')\n",
    "\n",
    "# Plot validation data\n",
    "plt.plot(X_val.flatten(), y_val,\n",
    "         label=\"Validation Data\",\n",
    "         marker='s',\n",
    "         linestyle='',\n",
    "         markersize=10,\n",
    "         color='green')\n",
    "\n",
    "# Plot test data\n",
    "plt.plot(X_test.flatten(), y_test,\n",
    "         label=\"Test Data\",\n",
    "         marker='x',\n",
    "         linestyle='',\n",
    "         markersize=12,\n",
    "         color='red')\n",
    "\n",
    "# Plot model prediction\n",
    "plt.plot(x_pred_lin, y_pred_lin,\n",
    "         label=\"Model Data NN\",\n",
    "         linewidth=2,\n",
    "         linestyle=\"--\",\n",
    "         color='black')\n",
    "\n",
    "plt.xlabel(\"x [-]\", fontsize=12)\n",
    "plt.ylabel(\"y [-]\", fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, frameon=False)\n",
    "plt.grid(True, linewidth=0.8, alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train 4 instances of the model and store predictions -----\n",
    "num_models = 4\n",
    "models = []\n",
    "all_predictions = []\n",
    "\n",
    "print(\"Training multiple model instances...\")\n",
    "\n",
    "for i in range(num_models):\n",
    "    print(f\"Training model {i+1}/{num_models}\")\n",
    "    \n",
    "    # Create a new instance of the same model architecture\n",
    "    model_instance = build_model()\n",
    "    \n",
    "    model_instance.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mae\"  # mean absolute error\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history_instance = model_instance.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        verbose=0  # suppress output for cleaner display\n",
    "    )\n",
    "    \n",
    "    # Store the trained model\n",
    "    models.append(model_instance)\n",
    "    \n",
    "    # Generate predictions for all data points\n",
    "    y_pred_all_instance = model_instance.predict(x_pred_lin, verbose=0).flatten()\n",
    "    all_predictions.append(y_pred_all_instance)\n",
    "    \n",
    "    # Calculate and display MAE for this instance\n",
    "    y_pred_test_instance = model_instance.predict(X_test, verbose=0).flatten()\n",
    "    mae_test_instance = mean_absolute_error(y_test, y_pred_test_instance)\n",
    "    print(f\"  Model {i+1} Test MAE: {mae_test_instance:.3f}\")\n",
    "\n",
    "print(f\"\\nCompleted training {num_models} model instances.\")\n",
    "print(f\"Models stored in 'models' list\")\n",
    "print(f\"Predictions stored in 'all_predictions' list\")\n",
    "print(f\"Shape of each prediction array: {all_predictions[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a24162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Plot individual predictions and calculate variance ----------\n",
    "# Convert predictions to numpy array for easier manipulation\n",
    "all_predictions_array = np.array(all_predictions)  # Shape: (num_models, num_data_points)\n",
    "\n",
    "\n",
    "# Calculate variance across models for each x value\n",
    "prediction_variance = np.var(all_predictions_array, axis=0)\n",
    "mean_variance = np.mean(prediction_variance)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot experimental data\n",
    "plt.plot(X_all, y_all, \n",
    "         label=\"Experimental Data\", \n",
    "         linewidth=3, \n",
    "         color='black', \n",
    "         zorder=10)\n",
    "\n",
    "# Plot individual model predictions\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i in range(num_models):\n",
    "    plt.plot(x_pred_lin, all_predictions_array[i], \n",
    "             label=f\"Model {i+1}\", \n",
    "             linewidth=2, \n",
    "             linestyle=\"--\", \n",
    "             color=colors[i],\n",
    "             alpha=0.8)\n",
    "\n",
    "# Calculate and plot ensemble mean\n",
    "ensemble_mean = np.mean(all_predictions_array, axis=0)\n",
    "plt.plot(x_pred_lin, ensemble_mean, \n",
    "         label=\"Ensemble Mean\", \n",
    "         linewidth=3, \n",
    "         linestyle=\"-.\", \n",
    "         color='purple',\n",
    "         zorder=5)\n",
    "\n",
    "plt.xlabel(\"t [s]\", fontsize=20)\n",
    "plt.ylabel(\"y [-]\", fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=14, frameon=False)\n",
    "plt.grid(True, linewidth=0.8, alpha=0.6)\n",
    "plt.title(f\"Individual Model Predictions\\nMean Variance: {mean_variance:.6f}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print variance statistics\n",
    "print(f\"Variance Statistics:\")\n",
    "print(f\"Mean variance across all x values: {mean_variance:.6f}\")\n",
    "print(f\"Min variance: {np.min(prediction_variance):.6f}\")\n",
    "print(f\"Max variance: {np.max(prediction_variance):.6f}\")\n",
    "print(f\"Std of variances: {np.std(prediction_variance):.6f}\")\n",
    "\n",
    "# Show where the highest variance occurs\n",
    "max_var_idx = np.argmax(prediction_variance)\n",
    "print(f\"\\nHighest variance at x = {x_pred_lin[max_var_idx, 0]:.3f}\")\n",
    "print(f\"Variance value: {prediction_variance[max_var_idx]:.6f}\")\n",
    "print(f\"Predictions at this point: {all_predictions_array[:, max_var_idx]}\")\n",
    "\n",
    "# Calculate ensemble MAE\n",
    "ensemble_mae_test_idx = np.isin(X_all.flatten(), X_test.flatten())\n",
    "ensemble_mean_test = ensemble_mean[sort_idx][ensemble_mae_test_idx[sort_idx]]\n",
    "y_test_sorted_for_ensemble = y_sorted[ensemble_mae_test_idx[sort_idx]]\n",
    "ensemble_mae = mean_absolute_error(y_test_sorted_for_ensemble, ensemble_mean_test)\n",
    "print(f\"\\nEnsemble Test MAE: {ensemble_mae:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
