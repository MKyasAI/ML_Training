{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3467b904-035b-403a-b9a2-3fcf5cf1bd11",
   "metadata": {},
   "source": [
    "# Aeroblade Deep Learning Project\n",
    "\n",
    "### Import libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a5635-89fb-488e-8ccf-458784e01d1d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --user pandas numpy scikit-learn tensorflow matplotlib ipywidgets seaborn scipy\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Normalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "from ipywidgets import interact, widgets\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a465beb-582e-4845-b7ba-379dcd9a45bc",
   "metadata": {},
   "source": [
    "### Read simulation data\n",
    "The data is read from a csv file into a pandas dataframe.\n",
    "The pandas dataframe is one of the standard input formats for most machine learning frameworks in python.\n",
    "Columns from the csv file are stored in named columns in the dataframe.\n",
    "Columns can be accessed by name or by index.\n",
    "Rows can be accessed by index.\n",
    "\n",
    "We print the dataframe to see if we imported the csv file correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb5542-f0a2-4c7f-9789-b92759d7f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"k_aeroblade.csv\"  # Update this path Nafems/k_aeroblade.csv\n",
    "print(file_path)\n",
    "df = pd.read_csv(file_path, delimiter = ';', decimal = ',')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d4b55-042f-4a28-947c-75b75d4ae0c9",
   "metadata": {},
   "source": [
    "### Data Review\n",
    "seaborn (sns) is used to create plots for data analysis.\n",
    "\n",
    "#### Histograms\n",
    "Histograms show the distribution of values for one column in the dataframe.\n",
    "the code creates a histogram that counts the number of values that fall between 0 and 1, 1 and 2, 2 and 3\n",
    "Histograms are used to detect imbalances in the dataset.\n",
    "\n",
    "#### Correlation Matrices\n",
    "Correlation matrices match every column of the dataframe with each other and calculate the correlation.\n",
    "**pearson** is used for linear correlations.\n",
    "**spearman** is used for simple nonlinear correlations.\n",
    "Parameters that strongly correlate with one another should not be used together as model inputs.\n",
    "Input parameters which correlate with output parameters should be used as inputs for the model. But weak correlation in the matrix does not mean a parameter should not be used as input, since the actual correlations might be more complex and depend on multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7510dd3e-b1c7-4a94-96d9-0f6ab5349392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['k10'], bins = [0, 1, 2, 3])\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(df.corr(method='spearman'), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a9077-2a6a-426d-88fe-18c2829660ae",
   "metadata": {},
   "source": [
    "#### Data Split\n",
    "First, we remove the columns we don't want to use from the dataset using drop.\n",
    "In this case, we split the data manually, and assign the rows systematically. We can do that because we know the data was randomly sampled. If the data was sampled systematically, we would use a randomizer to assign datapoints to the split sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3d5d7-4594-4f26-a5cc-cb722368ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['k1', 'k3',])\n",
    "training_data = df.iloc[0:6]\n",
    "val_data = df.iloc[6:8]\n",
    "test_data = df.iloc[8:]\n",
    "print(training_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac4b81-a2d9-48d1-b85e-cf375f4e7ab9",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "This bracket determines the neural networks architecture. <br>\n",
    "First, the training inputs are selected are selected and stored in a separate variable.\n",
    "The training inputs are then used to calibrate a **normalization layer** for the neural network. <br>\n",
    "The normalization layer transforms all incoming data, so that the originally selected input has a mean of 0 and a standard deviation of 1. <br>\n",
    "\n",
    "Inside **Sequential** we define the model architecture itself. Using Sequential, we can define as many layers as necessary in a simple sequence. <br>\n",
    "The current model has one predefined **input layer** matching our input data. <br>\n",
    "It is followed by the **normalization layer**, which we calibrated earlier. <br>\n",
    "The next three lines are deactivated (turned into comments). To activate them remove the # sign.<br>\n",
    "**Dense** layers are densely connected feedforward layers. The first number sets the number of neurons for that layer, activation sets the activation function for that layer. <br>\n",
    "One of the dense layers has a regularizer. Regularizers prevent overfitting, by forcing the optimizer to use small values for the neural networks weights. <br>\n",
    "The final dense layer uses a linear activation function (the same as no activation function) and serves as the networks output layer. <br>\n",
    "One of the comments is a **dropout** layer. Dropout layers mitigate overfitting, by randomly deactivating a fraction of the following layers neurons. <br><br>\n",
    "\n",
    "Inside compile the optimizer and the loss function are set. Metrics use loss functions to evaluate model performance and accuracy, but are not used for training. <br>\n",
    "Useful optimizers for this case are: 'adam', and 'SGD' (stochastic gradient descent) <br>\n",
    "Useful loss functions are 'mse' (mean squared error), 'mae' (mean absolute error), 'mape' (mean absolute percentage error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ffb8f-b9b5-4766-8a9c-27eefef8f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = training_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]]\n",
    "\n",
    "def newmodel(training_input):\n",
    "    normalization = Normalization()\n",
    "    normalization.adapt(training_input.to_numpy())\n",
    "    \n",
    "    model = Sequential([   \n",
    "        Input(shape=(training_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]].shape[1],)),  \n",
    "        normalization,\n",
    "        #Dense(10, activation='relu'),\n",
    "        #Dense(10, activation='relu'),\n",
    "        #Dense(10, activation='relu'),\n",
    "        #keras.layers.Dropout(0.5),\n",
    "        Dense(32, activation='relu', kernel_regularizer= keras.regularizers.l1(0.1)),\n",
    "        Dense(1)  # Output layer for regression (predicting downforce)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mape'])\n",
    "    return model\n",
    "\n",
    "model = newmodel(training_input)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc2c74-7ea6-4975-95bc-5db039d975c4",
   "metadata": {},
   "source": [
    "#### Fitting and Evaluation\n",
    "This brackets trains the model. First the training and the validation datasets are defined. <br>\n",
    "The most important parameter here is **epochs**. This is the number of training iterations for the optimizer. Increase epochs if the model does not fit properly. Decrease if it overfits. <br>\n",
    "Run this bracket multiple times to see random initialization changes the model in each training attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed56b0-3bc4-4ad6-b6f5-0a0e3a9d7a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = newmodel(training_input)\n",
    "history = model.fit(\n",
    "    training_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]], training_data[['k10']],\n",
    "    validation_data=(val_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]], val_data[['k10']]),\n",
    "    epochs=800,\n",
    "    batch_size=6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1fa68-0f8f-4da5-9f99-41b4f40e29b0",
   "metadata": {},
   "source": [
    "#### Test Evaluation\n",
    "This bracket evalutes the model with the selected test data. Check this metric only after settling on a preliminary model architecture to avoid model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3ace8-3cd1-4957-adba-962b533f8e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_metrics = model.evaluate(test_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]], test_data['k10'])\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacccfff-8c85-4905-a6fa-57e20d45d57d",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "This line of code predicts and prints the k10 value for the selected mesh configuration. This how the ML model is used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b53ee9-8c93-4814-989c-9b95643124b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([[5,0,0,0,5,0,0,0,0,5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88001bce-3c90-45f2-8098-22bdc8c49fa5",
   "metadata": {},
   "source": [
    "#### Active Sampling\n",
    "This is a very simple approach to active sampling. We create 10 models of the same architecture and predict the k10 value of 1000 different configurations. We calculate the variance of the predictions to see where the models disagree the most (query-by-comittee). That configuration is what we would simulate next and add to our dataset. <br>\n",
    "(Please note that this approach to active sampling is simplified to reduce computational time)\n",
    "\n",
    "\n",
    "The formula for sample variance is:\n",
    "\n",
    "$$\n",
    "S^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n - 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6da75f-bf39-497b-8a6d-115e9b05c693",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples and the shape of each sample\n",
    "num_samples = 1000\n",
    "sample_shape = 10\n",
    "\n",
    "# Generate 1000 random samples with integers between 0 and 5 (inclusive)\n",
    "np.random.seed(42)\n",
    "input_data = np.random.randint(0, 6, size=(num_samples, sample_shape))\n",
    "\n",
    "print(input_data)\n",
    "\n",
    "models = []\n",
    "for i in range(10):\n",
    "    model = newmodel(training_input)\n",
    "    model.fit(\n",
    "    training_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]], training_data[['k10']],\n",
    "    validation_data=(val_data[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10',]], val_data[['k10']]),\n",
    "    epochs=200,\n",
    "    #batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "    models.append(model)\n",
    "\n",
    "# Generate predictions for each model\n",
    "all_predictions = []\n",
    "\n",
    "for model in models:\n",
    "    # Get the predictions for the 1000 inputs\n",
    "    predictions = model.predict(input_data)\n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "# Convert the list of predictions to a numpy array\n",
    "# Shape will be (10, 1000, output_shape) if output is multi-dimensional, otherwise (10, 1000)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Calculate variance along the axis of models (axis=0)\n",
    "# This calculates the variance for each of the 1000 inputs across the 10 models\n",
    "variance = np.var(all_predictions, axis=0)\n",
    "\n",
    "# Find the index of the maximum variance\n",
    "max_variance_index = np.argmax(variance)\n",
    "\n",
    "# Retrieve the value of the maximum variance\n",
    "max_variance_value = variance[max_variance_index]\n",
    "\n",
    "# Retrieve the input vector at this index\n",
    "input_vector_at_max_variance = input_data[max_variance_index]\n",
    "\n",
    "#Calculate the mean variance per input\n",
    "mean_variance = np.mean(variance)\n",
    "# Print the results\n",
    "print(f\"Mean Configuration Variance: {mean_variance}\")\n",
    "print(f\"Median Configuration Variance: {np.median(variance)}\")\n",
    "print(f\"Maximum Configuration Variance Value: {max_variance_value}\")\n",
    "print(f\"Maximum Variance Configuration: {input_vector_at_max_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95600f64-43c0-41e5-8809-2e914f27b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(variance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e8ba1-e56f-4246-9bc8-53ebbb806a91",
   "metadata": {},
   "source": [
    "#### Save and Load\n",
    "The next brackets are used to save a model to a file and load it again in another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58be4a-ea05-4c62-b7ff-ec90e625d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b86d5a-d8f9-49ba-aa33-a0ad751abea8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('model.keras')\n",
    "print(loaded_model.predict(np.array([[0,0,5,3,0,5,0,0,0,5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e851e-42de-4f42-91d4-9610a7c0534d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Additional SVM solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR  # Support Vector Regressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = training_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']]\n",
    "training_target = training_data['k10']\n",
    "val_input = val_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']]\n",
    "val_target = val_data['k10']\n",
    "test_input = test_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']]\n",
    "test_target = test_data['k10']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "training_input_scaled = scaler.fit_transform(training_input)\n",
    "val_input_scaled = scaler.transform(val_input)\n",
    "test_input_scaled = scaler.transform(test_input)\n",
    "\n",
    "# Train SVM\n",
    "svm_model = SVR(epsilon=0.1, max_iter=-1)\n",
    "svm_model.fit(training_input_scaled, training_target)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = svm_model.predict(val_input_scaled)\n",
    "val_mse = mean_squared_error(val_target, val_predictions)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions = svm_model.predict(test_input_scaled)\n",
    "test_mse = mean_squared_error(test_target, test_predictions)\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac759ea9-0d94-43d1-b60c-c8fa5da6e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = df[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']]\n",
    "\n",
    "\n",
    "pca_only = PCA()#(n_components=2)\n",
    "\n",
    "features_pca_only = pca_only.fit_transform(features)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_pca_only[:, 0], features_pca_only[:, 1], cmap='viridis', s=50, alpha=0.7)\n",
    "plt.title(\"PCA Visualization of Features (x1 to x10)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "n_clusters = 3  # Number of clusters to use\n",
    "kmeans_for_features = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels_for_features = kmeans_for_features.fit_predict(features)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_pca_only[:, 0], features_pca_only[:, 1], c=kmeans_labels_for_features, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(kmeans_for_features.cluster_centers_[:, 0], kmeans_for_features.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title(\"K-means Clustering with PCA Visualization\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3b35c-9e41-4b04-a5f1-a5da76a3d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
